{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc004c2ecb404a2c8ef58f82e10ef4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e63c6fedefdb47ab85c1ba526ab00779",
              "IPY_MODEL_a9f780268da644adb69eff8fbbb85e34",
              "IPY_MODEL_53ceeac4f800425282ef9155119543cb"
            ],
            "layout": "IPY_MODEL_b7d92db338c24aa98e5607c521cb56da"
          }
        },
        "e63c6fedefdb47ab85c1ba526ab00779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7728ae78714a8dab567c07b9cca6e0",
            "placeholder": "​",
            "style": "IPY_MODEL_8a039cda400a42c3bc02cb3ab7d6426b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a9f780268da644adb69eff8fbbb85e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93bb88d509d46968446754548674e81",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7b66ecebd9f4420a765605af437ec51",
            "value": 2
          }
        },
        "53ceeac4f800425282ef9155119543cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_699a4c397a124385aa5e69723c412196",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf7246d72cc4b4eb2c4f4312c4cae92",
            "value": " 2/2 [00:49&lt;00:00, 23.62s/it]"
          }
        },
        "b7d92db338c24aa98e5607c521cb56da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7728ae78714a8dab567c07b9cca6e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a039cda400a42c3bc02cb3ab7d6426b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c93bb88d509d46968446754548674e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b66ecebd9f4420a765605af437ec51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "699a4c397a124385aa5e69723c412196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf7246d72cc4b4eb2c4f4312c4cae92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583849b2678c4751bb16743e5c880ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90b5f1ec63514ffdb3d88f252f3cd27d",
              "IPY_MODEL_96af53128d1f4a89aede9196375ef8ed",
              "IPY_MODEL_0f264ab9cb904553aba24a7a6ae6ad2f"
            ],
            "layout": "IPY_MODEL_0d792c21df854b80a750f0026e6ca70b"
          }
        },
        "90b5f1ec63514ffdb3d88f252f3cd27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_521aca4ae74d4c9ca05af17d99c06408",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3db6290a0541318f74af95994022e9",
            "value": "tokenizer.model: 100%"
          }
        },
        "96af53128d1f4a89aede9196375ef8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29ea61780224ad887f00cee69a56997",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7799108b1694867a0d68f8053967a8b",
            "value": 499723
          }
        },
        "0f264ab9cb904553aba24a7a6ae6ad2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94125565d8f24f62b5b173b5af567efb",
            "placeholder": "​",
            "style": "IPY_MODEL_919d034b9ae94a5bb9c39ee4824d4ad9",
            "value": " 500k/500k [00:00&lt;00:00, 17.7MB/s]"
          }
        },
        "0d792c21df854b80a750f0026e6ca70b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521aca4ae74d4c9ca05af17d99c06408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3db6290a0541318f74af95994022e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f29ea61780224ad887f00cee69a56997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7799108b1694867a0d68f8053967a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94125565d8f24f62b5b173b5af567efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919d034b9ae94a5bb9c39ee4824d4ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viswapani/Blackelephant/blob/main/Chatbot_LLaMa_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this Colab Notebook, we are going to explore Llama-2 7B, a model fine-tuned for generating text & chatting.\n",
        "\n",
        "By the end of this tutorial, you'll be able to interact with this model and use it to generate conversational responses.\n",
        "\n",
        "Whether you're curious about chatbot technology or simply want to see a machine-generated response to a particular question, this notebook will serve as a comprehensive guide.\n",
        "\n",
        "## Workflow\n",
        "1. **Installations**: We'll begin by setting up our environment with the required libraries.\n",
        "2. **Prerequisites**: Ensure we have access to the Llama-2 7B model on Hugging Face.\n",
        "3. **Loading the Model & Tokenizer**: Retrieve the model and tokenizer for our session.\n",
        "4. **Creating the Llama Pipeline**: Prepare our model for generating responses.\n",
        "5. **Interacting with Llama**: Prompt the model for answers and explore its capabilities.\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "**First, change runtime to GPU.**\n",
        "\n",
        "\n",
        "You can play with Llama-2 7B Chat here: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat"
      ],
      "metadata": {
        "id": "bWOx7lgoYHZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnYTe5R_D4bE",
        "outputId": "5fef7144-b931-4660-ce6c-ddf8231ff102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar  8 18:41:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0             28W /   70W |   13026MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whD8Q_L95UPo",
        "outputId": "ea4d28d6-4ab2-4415-d620-8cd88e21f1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   40G   74G  35% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.7G  4.0K  5.7G   1% /dev/shm\n",
            "/dev/root       2.0G  1.2G  820M  59% /usr/sbin/docker-init\n",
            "/dev/sda1       119G  115G  4.3G  97% /opt/bin/.nvidia\n",
            "tmpfs           6.4G  252K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!~/.cache/huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfleZ7Rb5lR6",
        "outputId": "ac5600db-1995-4119-fd09-56d4e9a65d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /root/.cache/huggingface: Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.cache/huggingface\n"
      ],
      "metadata": {
        "id": "ha1UT5Yl6Gh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/drive/MyDrive/LlamaModels\"\n"
      ],
      "metadata": {
        "id": "9-ZglLi06Y8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87989c2-82cd-4074-828b-933821f53673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "1. Gain access to the model on Hugging Face: [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n",
        "2. Use the Hugging Face CLI to login and verify your authentication status.\n",
        "\n"
      ],
      "metadata": {
        "id": "BO2pb-EeZA95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_6AfgF3_arYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be31667-7858-4973-f402-11cbb44e8cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `llama2key` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llama2key`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "6lD_oW1uavGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb01553-509f-4734-a4f0-a6e957d34b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viswapani\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "model = \"meta-llama/Llama-2-7b-hf\" #\"meta-llama/Llama-2-7b-hf-chat\" #\"meta-llama/Llama-3.3-70B-Instruct\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "583849b2678c4751bb16743e5c880ef6",
            "90b5f1ec63514ffdb3d88f252f3cd27d",
            "96af53128d1f4a89aede9196375ef8ed",
            "0f264ab9cb904553aba24a7a6ae6ad2f",
            "0d792c21df854b80a750f0026e6ca70b",
            "521aca4ae74d4c9ca05af17d99c06408",
            "4c3db6290a0541318f74af95994022e9",
            "f29ea61780224ad887f00cee69a56997",
            "e7799108b1694867a0d68f8053967a8b",
            "94125565d8f24f62b5b173b5af567efb",
            "919d034b9ae94a5bb9c39ee4824d4ad9"
          ]
        },
        "outputId": "413c3d10-a479-44d5-f20b-3a8327e36dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "583849b2678c4751bb16743e5c880ef6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype= \"auto\",              #torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "0AqJo1R_a9IM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "bc004c2ecb404a2c8ef58f82e10ef4f2",
            "e63c6fedefdb47ab85c1ba526ab00779",
            "a9f780268da644adb69eff8fbbb85e34",
            "53ceeac4f800425282ef9155119543cb",
            "b7d92db338c24aa98e5607c521cb56da",
            "ad7728ae78714a8dab567c07b9cca6e0",
            "8a039cda400a42c3bc02cb3ab7d6426b",
            "c93bb88d509d46968446754548674e81",
            "f7b66ecebd9f4420a765605af437ec51",
            "699a4c397a124385aa5e69723c412196",
            "dbf7246d72cc4b4eb2c4f4312c4cae92"
          ]
        },
        "outputId": "bb883101-b4aa-4262-984a-3bb69399056f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc004c2ecb404a2c8ef58f82e10ef4f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Responses\n",
        "\n",
        "With everything set up, let's see how Llama responds to some sample queries."
      ],
      "metadata": {
        "id": "tlbsuGikcsB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "m8RwW7Axcu9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a731aee-92aa-42ca-af68-2739ba6566b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
            "I've been watching \"The Wire\" and I'm really enjoying it. It's a little slow at times, but I think it's worth it.\n",
            "I'm also a big fan of \"The Shield\" and \"The Sopranos\".\n",
            "I'm currently watching \"House of Cards\". It's a great show.\n",
            "I've been watching \"The Wire\" and I'm really enjoying it.\n",
            "I'm also a big fan of \"The Shield\" and \"The Sopranos\". I'm currently watching \"House of Cards\". It's a great show.\n",
            "I've been watching \"The Wire\" and I'm really enjoying it. It's a little slow at times, but I think it's worth it. I'm also a big fan of \"The Shield\" and \"The Sopranos\". I'm currently watching \"House of Cards\". It's a great show.\n",
            "I'm a big fan of \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Queries"
      ],
      "metadata": {
        "id": "7HSx6ctIdbW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "FoYglrLvmTN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ada3f89-e8c6-48ec-d002-e822bb2897df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.Based on that, what language should I learn next?Give me 5 recommendations.\n",
            "I would recommend learning Java, because it is one of the most popular languages in the industry. It is also very well suited for building web applications.\n",
            "I would also recommend learning C#, because it is a very popular language and it is very well suited for building Windows applications.\n",
            "I would also recommend learning C++, because it is a very popular language and it is very well suited for building desktop applications.\n",
            "I would also recommend learning JavaScript, because it is a very popular language and it is very well suited for building web applications.\n",
            "I would also recommend learning PHP, because it is a very popular language and it is very well suited for building web applications.\n",
            "I would also recommend learning Python, because it is a very popular language and it is very well suited for building web applications.\n",
            "I would also recommend learning Ruby, because it is a very popular language and it is very well suited for building web applications.\n",
            "I would also recommend learning Swift, because it is a very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to learn fast?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "hk3QokHDdgx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba09df42-a978-4644-ea33-7f5a7f56eb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to learn fast?\n",
            "What are the best ways to learn fast?\n",
            "How can I learn faster in school?\n",
            "How can I learn faster in college?\n",
            "How can I learn faster in university?\n",
            "How can I learn faster in high school?\n",
            "How can I learn faster in middle school?\n",
            "How can I learn faster in elementary school?\n",
            "How can I learn faster in preschool?\n",
            "How can I learn faster in kindergarten?\n",
            "How can I learn faster in my first grade?\n",
            "How can I learn faster in second grade?\n",
            "How can I learn faster in third grade?\n",
            "How can I learn faster in fourth grade?\n",
            "How can I learn faster in fifth grade?\n",
            "How can I learn faster in sixth grade?\n",
            "How can I learn faster in seventh grade?\n",
            "How can I learn faster in eighth grade?\n",
            "How can I learn faster in ninth grade?\n",
            "How can I learn faster in tenth grade?\n",
            "How can I learn faster in eleventh grade?\n",
            "How can I learn faster in twelfth grade?\n",
            "How can I learn faster in high school?\n",
            "How can I learn faster in college?\n",
            "How can I learn faster in university?\n",
            "How can I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I love basketball. Do you have any recommendations of team sports I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "HNCpOybWdc65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0289c6e-58bf-4023-bd03-bfa7438cccd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I love basketball. Do you have any recommendations of team sports I might like?\n",
            "I'm not a fan of team sports. I've never been good at team sports. I'm a very competitive person, and I don't like to lose. I like to win. I'm a good team player, but I like to win. I don't like to lose.\n",
            "I'm a very competitive person, and I don't like to lose. I like to win. I'm a good team player, but I like to win. I don't like to lose.\n",
            "I'm a very competitive person, and I don't like to lose. I like to win. I'm a good team player, but I like to win. I don't like to lose.\n",
            "I'm a very competitive person, and I don't like to lose. I like to win. I'm a good team player, but I like to win. I don't like to lose. I'm a very competitive person, and I don't like to lose. I like to win. I'm a good team player, but I like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to get rich?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "kzG2YoH4dil_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efcb029-3b04-45fb-df4f-9163d443be3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to get rich?\n",
            "Invest in the future.\n",
            "What is the future of the world?\n",
            "The future is in the hands of the youth.\n",
            "How to invest in the future?\n",
            "Invest in the youth.\n",
            "What is the best way to invest in the youth?\n",
            "The best way to invest in the youth is to support them in their education.\n",
            "How to support the youth in their education?\n",
            "There are many ways to support the youth in their education. Some of the most common ways include providing scholarships, financial aid, and mentorship.\n",
            "What are some of the best ways to support the youth in their education?\n",
            "Some of the best ways to support the youth in their education include providing scholarships, financial aid, and mentorship.\n",
            "What are some of the best ways to support the youth in their education?\n",
            "Some of the best ways to support the youth in their education include providing scholarships, financial aid, and mentorship.\n",
            "What are some of the best ways to support the youth in their education? Some of the best ways to support the youth in their education include providing scholarships, financial aid, and mentorship.\n",
            "How to support the youth in their education?\n",
            "There\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems\n",
        "\n",
        "After 3-4 prompts, the model stops giving responses. It only outputs the user prompt.\n",
        "\n",
        "To keep talking to the model, you need to restart the notebook: `Runtime -> Restart Runtime` and run the notebook again..."
      ],
      "metadata": {
        "id": "xBfybeqxr3jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make it conversational\n",
        "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
        "\n",
        "Type your questions or comments, and see how the model responds!"
      ],
      "metadata": {
        "id": "De1MFB-xgavO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    get_llama_response(user_input)"
      ],
      "metadata": {
        "id": "a4xgy3NMgcwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3702ef6d-18f2-4921-9822-55364973e899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: what is today?\n",
            "Chatbot: what is today?\n",
            "I am so tired. I am so tired of being tired. I am so tired of being tired of being tired. I am so tired of being tired of being tired of being tired. I am so tired of being tired of being tired of being tired of being tired. I am so tired of being tired of being tired of being tired of being tired of being tired. I am so tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired. I am so tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired. I am so tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being tired of being\n",
            "You: what is capital of France?\n",
            "Chatbot: what is capital of France?\n",
            "Capital of France?\n",
            "What is the capital of France?\n",
            "What is the capital of France called?\n",
            "What is the capital of France in English?\n",
            "What is the capital of France called in French?\n",
            "What is the capital of France called in French?\n",
            "What is the capital of France called in French?\n",
            "What is the capital of France called in French?\n",
            "What is the capital of France in French?\n",
            "What is the capital of France in French?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in English?\n",
            "What is the capital of France called in\n",
            "You: what is capital city of the country France?\n",
            "Chatbot: what is capital city of the country France?\n",
            "Capital city of France is Paris.\n",
            "what is capital city of the country France?\n",
            "What is the capital city of France?\n",
            "The capital of France is Paris.\n",
            "What is the capital city of the country France?\n",
            "What is the capital city of France?\n",
            "What is the capital of France?\n",
            "What is the capital city of France?\n",
            "What is the capital city of France called?\n",
            "What is the capital of the country France?\n",
            "What is the capital of the country France?\n",
            "What is the capital of France?\n",
            "What is the capital of France called?\n",
            "What is the capital of France called?\n",
            "What is the capital of France called?\n",
            "What is the capital city of the country France?\n",
            "What is the capital of the country France?\n",
            "What is the capital of the country France called?\n",
            "What is the capital of the country France?\n",
            "What is the capital city of France called?\n",
            "What is the capital city of the country France called?\n",
            "What is the capital city of France called?\n",
            "What is the capital city of the country France called?\n",
            "What is the capital city of the country France?\n",
            "What is the capital city of France called?\n",
            "\n",
            "You: why is that you are not able to give correct answer \"paris\"?\n",
            "Chatbot: why is that you are not able to give correct answer \"paris\"?\n",
            "Why is that you are not able to give correct answer \"paris\"?\n",
            "Why is that you are not able to give correct answer \"paris\"? Paris is the capital city of France. It is the most populous city in France.\n",
            "Why is that you are not able to give correct answer \"paris\"? Paris is the capital city of France. It is the most populous city in France. Paris is located on the river Seine. It is the most romantic city in the world. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very beautiful city. Paris is a very beautiful city. It is a very\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5OW9VH2lVb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Thanks to the Hugging Face Library, creating a pipeline to chat with llama 2 (or any other open-source LLM) is quite easy.\n",
        "\n",
        "But if you worked a lot with much larger models such as GPT-4, you need to adjust your expectations."
      ],
      "metadata": {
        "id": "s3lxvhWKqfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWD3HFWlr2BE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}